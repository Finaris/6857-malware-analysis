# -*- coding: utf-8 -*-
"""

This module implements the Scraper interface for malc0de for
data mining purposes.

"""

import logging

from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List

from scraper import Scraper


class MalcodeScraper(Scraper):
    """ Implementation of Scraper for malc0de. """

    def __init__(self):
        # Specify the URL of malc0de and its pages we are scanning.
        self.short_name = "malcode"
        self.base_url = "http://malc0de.com/"
        self.pages_scraped = ["database/?&page={0}".format(i) for i in range(1, 5)]  # Pages are 1-5.
        super().__init__(self.short_name, self.base_url, self.pages_scraped)

        # Clear the remaining class attributes.
        self.records = {}
        self.timestamp = None

    def scrape(self):
        # Before beginning, clear all records, update the timestamp, and set up logging.
        self.records = {}
        self.timestamp = datetime.now()

        # Add records for each page by fetching the page and stripping all data.
        for page_name in self.pages_scraped:
            # Get a response from making a request to the page and only proceed if the request worked.
            url = self.base_url + page_name
            response, is_ok = self.make_request(url)
            if not is_ok:
                continue

            # Parse the response's content and store the records.
            html = BeautifulSoup(response.content, 'html.parser')
            self.records[page_name] = self._generate_records(url, html)
        logging.info("Completed scraping {0}".format(self.base_url))

    @staticmethod
    def _generate_records(url: str, html: BeautifulSoup) -> List[Dict]:
        """ Given a malc0de HTML page from the database, generate a list of records.

        :param url: (str) The current URL being viewed.
        :param html: (BeautifulSoup) HTML response from malc0de.
        :return: (list) A list of record dictionaries.
        """
        # Fetch the information table and all of its rows (exclude the header row).
        info_table = html.find("table")
        records = []
        for tr in info_table.select("tr")[1:]:
            # Find all tabular data. This isn't a strong check, but make sure the data is how we expect it.
            td = tr.find_all("td")
            if len(td) != 7:
                logging.error("Data not formatted as expected for component in {0}".format(url))
                continue

            # Load information from the tabular data.
            date = str(datetime.strptime(td[0].text, "%Y-%m-%d"))
            malware_url = td[1].text
            ip = td[2].text
            cc = td[3].text
            asn = int(td[4].text)
            autonomous_system_name = td[5].text
            md5 = td[6].text

            # Store all of the loaded information into a new record.
            records.append({
                "date": date,
                "url": malware_url,
                "ip": ip,
                "cc": cc,
                "asn": asn,
                "autonomous_system_name": autonomous_system_name,
                "md5": md5,
                "category": Scraper.GENERAL_MALWARE
            })
        return records

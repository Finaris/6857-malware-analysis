# -*- coding: utf-8 -*-
"""

This module provides the skeleton of an implementable interface which all
individual scrapers will use. Each website will likely need its own type of
scraper as the format of blacklisted URLs and information contained might be
different for each website. This standardization will simplify data collection.

Below is a description of all record types and their corresponding value formats.

    - asn: Autonomous system number, store this as a number, e.g.:
                60307
    - autonomous_system_name: The name of the autonomous system as a string, e.g.:
                "HVFOPSERVER-AS, UA"
    - category: A general label for what type of malicious behavior is being performed
                at the URL (like general malware, phishing, fraud, etc.). Use one of the
                defined constants below, e.g.:
                self.SPAM
    - cc: The country code of the origin as a string, e.g. for the United States: "US"
    - date: The date that malware was detected at a URL. Convert this to a Python
            datetime object then save the string version of it, e.g.:
                "2019-04-30 14:53:49.447774"
    - description: This is usually a descriptive string, so just put it as is. It's going
                   to be impossible to generalize this so just store it as extra information.
                   For example:
                "redirects to Paypal phishing"
    - ip: The IP address (i.e. after DNS lookup) as a string of the URL, like:
                "127.0.0.1"
    - md5: An md5 string hash of the malware (which can also be used on VirusTotal), e.g.:
                "fc91018928d70bc4aa4fb15d1a1e1183"
    - origin_country: A string name of the country (this doesn't need to be
                      pre-formatted, just submit as is--we'll handle countries with
                      potentially multiple names when normalizing). This must be the
                      FULL name of the country, not a shorthand code. For example:
                "United States of America"
    - registrar: The registrar which is in charge of the domain. Provide as a string, e.g.:
                "Google LLC"
    - reverse_lookup: A string name of actual domain where the malware is hosted. This is
                      not super necessary since the IP is usually enough, but store it just
                      in case. For example:
                "malicious.domain.com."
    - status: The status of the website, if this is available. Some old websites will obviously
              be down. For example,
                "active"
    - url: URL of where the malware was found. Just put the full URL as a string, like:
                "https://www.google.com"

"""

import logging
import requests

from typing import Tuple


class Scraper:
    """ Interface that scrapers for a particular website should follow.

    Attributes:
        short_name: (str) A short, unique, human-readable string which identifies the
                          scraper. When making a scraper, please make it's filename:
                          "short_name_scraper.py" and its implementing class "ShortNameScraper".
        base_url: (str) The URL of the website which is being scraped. Include a
                         forward slash, e.g.: https://www.google.com/
        pages_scraped: (list) A list of strings containing the names of subdirectories
                              or pages that were scraped. For example, if you are scraping
                              https://www.google.com/page1/ and https://www.google.com/page2/,
                              the list will look like: ["page1/", "page2/"]
        records: (dict) A dictionary which maps a page name to a list of dictionaries where
                        each dictionary in this list represents a single record (i.e. piece
                        of malware). We map attributes of the malware to their corresponding
                        values. For example:

                            {'origin_country': 'france', 'ip': '127.0.0.1'}

                        Records will store information about every piece of malware
                        and group that information with the page it was found on.
                        For more information about these attributes, see the
                        documentation above. Records will store all such
                        dictionaries and group them with their respective pages.

                        Note: All data in dictionaries MUST be JSON serializable
                              (i.e. no Python objects, tuples, sets, etc.)
        timestamp: (datetime) Python datetime object of when scraping occurred.

    """

    # Constants for categories.
    EXPLOIT = "exploit"
    FRAUD = "fraud"
    GENERAL_MALWARE = "general malware"
    HIJACK = "hijack"
    MISLEADING_MARKETING = "misleading_marketing"
    PHISHING = "phishing"
    PUP = "pup"

    def __init__(self, short_name: str, url: str, pages_scraped: list) -> None:
        self.short_name = short_name
        self.base_url = url
        self.pages_scraped = pages_scraped
        self.records = {}
        self.timestamp = None
        self.logging_setup()

    def scrape(self) -> None:
        """ Given a particular page on the website, scrape it for relevant
            information. Make sure to scrape all necessary subdirectories or pages
            if necessary. Clear records every time this is called or else there
            may be duplicates (assuming this function is called more than once).
            Lastly, updates the timestamp of when this scraping operation began.

        :return: (None) This function mutates the records attribute.
        """
        raise NotImplementedError

    ##########################
    # Utility Instance Methods
    ##########################

    def logging_setup(self) -> None:
        """ Sets up the logger for file and stdout logging.

        :return: (None) Modifies the state of the logger.
        """
        # Set up the root logger which will output to stdout (will overwrite existing configs).
        root_logger = logging.getLogger()
        logging_format_string = '%(asctime)s %(levelname)s %(message)s'
        logging.basicConfig(format=logging_format_string, level=logging.INFO)
        log_formatter = logging.Formatter(logging_format_string)

        # Create a file handler for this particular instance of the scraper.
        file_handler = logging.FileHandler('logs/{0}.log'.format(self.short_name))
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(log_formatter)
        root_logger.addHandler(file_handler)

    # TODO: Add convenience methods for dumping and pickling.

    ##########################
    # Utility Static Methods
    ##########################

    @staticmethod
    def make_request(url: str) -> Tuple[requests.models.Response, bool]:
        """ Makes a simple GET request and validates that it worked correctly.

        :param url: (str) The URL we are fetching via a GET request.
        :return: (tuple) A tuple of the actual requests response object and a status
                         which is True if the request was okay and False otherwise.
        """
        try:
            response = requests.get(url)
            if response.status_code != 200:
                logging.error("Invalid response code for {0}: {1}".format(url, response.status_code))
                return response, False
            return response, True
        except requests.RequestException as e:
            logging.error("Encountered error while making request to {0}: {1}"
                          .format(url, str(e)))

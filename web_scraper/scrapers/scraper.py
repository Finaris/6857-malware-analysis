# -*- coding: utf-8 -*-
"""

This module provides the skeleton of an implementable interface which all
individual scrapers will use. Each website will likely need its own type of
scraper as the format of blacklisted URLs and information contained might be
different for each website. This standardization will simplify data collection.

Below is a description of all record types and their corresponding value formats.

    - origin_country: A string name of the country (this doesn't need to be
                      pre-formatted, just submit as is--we'll handle countries with
                      potentially multiple names when normalizing).

"""


class Scraper:
    """ Interface that scrapers for a particular website should follow.

    Attributes:
        base_url: (str) The URL of the website which is being scraped. Include a
                         forward slash, e.g.: https://www.google.com/
        pages_scraped: (list) A list of strings containing the names of subdirectories
                              or pages that were scraped. For example, if you are scraping
                              https://www.google.com/page1/ and https://www.google.com/page2/,
                              the list will look like: ["page1/", "page2/"]
        records: (dict) A dictionary which maps the name of a page being scraped (without the
                        base URL, just the name of the page) to a list of dictionaries of the
                        following format:
                            {'origin_country': {'france': 1, 'germany': 2}}

                        These dictionaries map some attribute to a frequency
                        dictionary of specific instances of this attribute.
                        For more information about these attributes, see the
                        documentation above. Records will store all such
                        dictionaries and group them with their respective pages.

                        Note: All data in dictionaries MUST be JSON serializable
                              (i.e. no Python objects, tuples, sets, etc.)
        timestamp: (datetime) Python datetime object of when scraping occurred.

    """

    def __init__(self, url: str) -> None:
        self.base_url = url
        self.pages_scraped = []
        self.records = []
        self.timestamp = None

    def scrape(self) -> None:
        """ Given a particular page on the website, scrape it for relevant
            information. Make sure to scrape all necessary subdirectories or pages
            if necessary. Clear records every time this is called or else there
            may be duplicates (assuming this function is called more than once).
            Lastly, updates the timestamp of when this scraping operation began.

        :return: (None) This function mutates the records attribute.
        """
        raise NotImplementedError

    # TODO: Add convenience methods for dumping and pickling.

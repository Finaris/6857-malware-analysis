# -*- coding: utf-8 -*-
"""

This module provides the skeleton of an implementable interface which all
individual scrapers will use. Each website will likely need its own type of
scraper as the format of blacklisted URLs and information contained might be
different for each website. This standardization will simplify data collection.

"""

import logging
import os
import requests
import socket
import struct

from typing import List, Tuple


class RecordAttribute:
    """ Enum-like container for record attributes. """

    # Autonomous system number, store this as a number, e.g.:
    #       60307
    ASN = "asn"

    # The name of the autonomous system as a string, e.g.:
    #       "HVFOPSERVER-AS, UA"
    AUTONOMOUS_SYSTEM_NAME = "autonomous_system_name"

    # A general label for what type of malicious behavior is being performed
    # at the URL (like general malware, phishing, fraud, etc.). Use one of the
    # defined constants below, e.g.:
    #       self.SPAM
    CATEGORY = "category"

    # The country code of the origin as a string, e.g. for the United States:
    #       "US"
    CC = "cc"

    # The date that malware was detected at a URL. Convert this to a Python
    # datetime object then save the string version of it, e.g.:
    #       "2019-04-30 14:53:49.447774"
    DATE = "date"

    # This is usually a descriptive string, so just put it as is. It's going
    # to be impossible to generalize this so just store it as extra information.
    # For example:
    #       "redirects to Paypal phishing"
    DESCRIPTION = "description"

    # If it's a piece of malware and the filetype is reported, store it. For example,
    #       "exe"
    FILETYPE = "filetype"

    # The IP address (i.e. after DNS lookup) as a string of the URL, like:
    #       "127.0.0.1"
    IP = "ip"

    # An md5 string hash of the malware (which can also be used on VirusTotal), e.g.:
    #       "fc91018928d70bc4aa4fb15d1a1e1183"
    MD5 = "md5"

    # A string name of the country (this doesn't need to be
    # pre-formatted, just submit as is--we'll handle countries with
    # potentially multiple names when normalizing). This must be the
    # FULL name of the country, not a shorthand code. For example:
    #       "United States of America"
    ORIGIN_COUNTRY = "origin_country"

    # The registrar which is in charge of the domain. Provide as a string, e.g.:
    #       "Google LLC"
    REGISTRAR = "registrar"

    # A string name of actual domain where the malware is hosted. This is
    # not super necessary since the IP is usually enough, but store it just
    # in case. For example:
    #       "malicious.domain.com."
    REVERSE_LOOKUP = "reverse_lookup"

    # A SHA256 string hash of the malware, e.g.:
    #       "01de61f4fd7dc2cf4e19c533b42f5ddf93d1c5116e374212480b088f0d2def4f"
    SHA256 = "sha256"

    # The status of the website, if this is available. Some old websites will obviously
    # be down. For example,
    #       "active"
    STATUS = "status"

    # URL of where the malware was found. Just put the full URL as a string, like:
    #       "https://www.google.com"
    URL = "url"


class Category:
    """ Container for categories of malicious hosted content. """

    BOTNET = "botnet"
    EXPLOIT = "exploit"
    FRAUD = "fraud"
    GENERAL_MALWARE = "general malware"
    HIJACK = "hijack"
    MISLEADING_MARKETING = "misleading_marketing"
    PHISHING = "phishing"
    PUP = "pup"
    RANSOMWARE = "ransomware"
    SPYWARE = "spyware"
    VIRUS = "virus"
    WORM = "worm"


class Scraper:
    """ Interface that scrapers for a particular website should follow.

    Attributes:
        short_name: (str) A short, unique, human-readable string which identifies the
                          scraper. When making a scraper, please make it's filename:
                          "short_name_scraper.py" and its implementing class "ShortNameScraper".
        base_url: (str) The URL of the website which is being scraped. Include a
                         forward slash, e.g.: https://www.google.com/
        pages_scraped: (list) A list of strings containing the names of subdirectories
                              or pages that were scraped. For example, if you are scraping
                              https://www.google.com/page1/ and https://www.google.com/page2/,
                              the list will look like: ["page1/", "page2/"]
        records: (dict) A dictionary which maps a page name to a list of dictionaries where
                        each dictionary in this list represents a single record (i.e. piece
                        of malware). We map attributes of the malware to their corresponding
                        values. For example:

                            {'origin_country': 'france', 'ip': '127.0.0.1'}

                        Records will store information about every piece of malware
                        and group that information with the page it was found on.
                        For more information about these attributes, see the
                        documentation above. Records will store all such
                        dictionaries and group them with their respective pages.

                        Note: All data in dictionaries MUST be JSON serializable
                              (i.e. no Python objects, tuples, sets, etc.)
        timestamp: (datetime) Python datetime object of when scraping occurred.

    """

    def __init__(self, short_name: str) -> None:
        self.short_name = short_name
        self.records = {}
        self.timestamp = None
        self.logging_setup()

    def scrape(self) -> None:
        """ Given a particular page on the website, scrape it for relevant
            information. Make sure to scrape all necessary subdirectories or pages
            if necessary. Clear records every time this is called or else there
            may be duplicates (assuming this function is called more than once).
            Lastly, updates the timestamp of when this scraping operation began.

        :return: (None) This function mutates the records attribute.
        """
        raise NotImplementedError

    ##########################
    # Utility Instance Methods
    ##########################

    def logging_setup(self) -> None:
        """ Sets up the logger for file and stdout logging.

        :return: (None) Modifies the state of the logger.
        """
        # Set up the root logger which will output to stdout (will overwrite existing configs).
        root_logger = logging.getLogger()
        logging_format_string = '%(asctime)s %(levelname)s %(message)s'
        logging.basicConfig(format=logging_format_string, level=logging.INFO)
        log_formatter = logging.Formatter(logging_format_string)

        # Create a file handler for this particular instance of the scraper.
        file_handler = logging.FileHandler('{0}/logs/{1}.log'
                                           .format(os.path.dirname(os.path.realpath(__file__)), self.short_name))
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(log_formatter)
        root_logger.addHandler(file_handler)

    ##########################
    # Utility Static Methods
    ##########################

    @staticmethod
    def make_request(url: str) -> Tuple[requests.models.Response, bool]:
        """ Makes a simple GET request and validates that it worked correctly.

        :param url: (str) The URL we are fetching via a GET request.
        :return: (tuple) A tuple of the actual requests response object and a status
                         which is True if the request was okay and False otherwise.
        """
        try:
            response = requests.get(url)
            if response.status_code != 200:
                logging.error("Invalid response code for {0}: {1}".format(url, response.status_code))
                return response, False
            return response, True
        except requests.RequestException as e:
            logging.error("Encountered error while making request to {0}: {1}"
                          .format(url, str(e)))

    @staticmethod
    def ips_in_range(start: str, end: str) -> List:
        """ Given a start and end IP address, generate a list of IP addresses in the range (inclusive).

        :param start: (str) Beginning IP address.
        :param end: (str) End IP address.
        :return: (list) List of all IP addresses.
        """
        first_address = struct.unpack('>I', socket.inet_aton(start))[0]
        last_address = struct.unpack('>I', socket.inet_aton(end))[0]
        return [socket.inet_ntoa(struct.pack('>I', i)) for i in range(first_address, last_address+1)]

    @staticmethod
    def is_url_ip_address(url: str) -> bool:
        """ Given something that looks like a URL, determine if it's an IP address or domain name.

        :param url: (str) String which looks like a URL.
        :return: (bool) True if it looks like an IP address, False otherwise.
        """
        return all(component.isdigit() for component in url.split("."))

# -*- coding: utf-8 -*-
"""

This module provides the skeleton of an implementable interface which all
individual scrapers will use. Each website will likely need its own type of
scraper as the format of blacklisted URLs and information contained might be
different for each website. This standardization will simplify data collection.

"""


class Scraper:
    """ Interface that scrapers for a particular website should follow.

    Attributes:
        url: (str) The URL of the website which is being scraped. Include a
                   forward slash, e.g.: https://www.google.com/
        records: (list) A list of dictionaries of the following format:
                            {'origin_country': {'france': 1, 'germany': 2}}

                        These dictionaries map some attribute to a frequency
                        dictionary of specific instances of this attribute.
                        Records will store all such dictionaries.

                        Note: All data in dictionaries MUST be JSON serializable
                              (i.e. no Python objects, tuples, sets, etc.)
        timestamp: (datetime) Python datetime object of when scraping occurred.

    """

    def __init__(self, url):
        self.url = url
        self.records = []
        self.timestamp = None

    def scrape(self, page=""):
        """ Given a particular page on the website, scrape it for relevant
            information. Clear records every time this is called or else there
            may be duplicates (assuming this function is called more than once).
            Lastly, updates the timestamp of when this scraping operation began.

        :param page: (str) Name of the page to scrape (homepage by default)
        :return: (None) This function mutates the records attribute.
        """
        raise NotImplementedError

    # TODO: Add convenience methods for dumping and pickling.

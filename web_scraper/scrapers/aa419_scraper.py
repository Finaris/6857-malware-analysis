# -*- coding: utf-8 -*-
"""

This module implements the Scraper interface for
AA419 for data mining purposes.

"""

import logging

from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict

from scraper import Category, RecordAttribute, Scraper


class Aa419Scraper(Scraper):
    """ Implementation of Scraper for AA419. """

    def __init__(self):
        # Specify the URL of AA419 and its pages we are scanning.
        self.short_name = "aa419"
        self.base_url = "https://db.aa419.org/"
        self.pages_scraped = ["fakebanksview.php?key={0}".format(i) for i in range(32617, 137556)]
        super().__init__(self.short_name, self.base_url, self.pages_scraped)

        # Clear the remaining class attributes.
        self.records = {}
        self.timestamp = None

    def scrape(self):
        # WARNING! This scrapes a looooot of pages, so we probably want to do this in
        # batches. We don't want to accidentally DoS the website or anything, so keep
        # that in mind before this job is run.
        # Before beginning, clear all records and update the timestamp.
        self.records = {}
        self.timestamp = datetime.now()

        # Scrape all of the pages on the actual list.
        for page_name in self.pages_scraped:
            # Get a response from making a request to the page and only proceed if the request worked.
            url = self.base_url + page_name
            response, is_ok = self.make_request(url)
            if not is_ok:
                continue

            # Parse the response's content and store the records.
            html = BeautifulSoup(response.content, 'html.parser')
            new_record = self._generate_record(url, html)
            if new_record:
                self.records[page_name] = new_record
        logging.info("Completed scraping {0}".format(self.short_name))

    @staticmethod
    def _generate_record(url: str, html: BeautifulSoup) -> Dict:
        """ Given an AA419 php page from the database, generate a list of records.

        :param url: (str) The current URL being viewed.
        :param html: (BeautifulSoup) HTML response from AA419.
        :return: (list) A list of record dictionaries.
        """
        # Get all the rows and verify the length is correct.
        info_table = html.find("table")
        all_rows = info_table.select("tr")
        if len(all_rows) != 14:
            logging.error("Data not formatted as expected for component in {0}".format(url))
            return {}

        # Create a new record and populate it accordingly.
        new_record = {
            RecordAttribute.URL: all_rows[2].find('td').text.strip(),
            RecordAttribute.IP: all_rows[5].find('td').text.strip(),
            RecordAttribute.ASN: int(all_rows[7].find('td').text.strip()),
            RecordAttribute.STATUS: all_rows[8].find('td').text.strip(),
            RecordAttribute.DATE: str(datetime.strptime(all_rows[10].find('td').text.strip(), "%Y-%m-%d %H:%M:%S")),
            RecordAttribute.CATEGORY: Category.FRAUD
        }

        # Add the registrar if it's there.
        registrar = all_rows[9].find('td').text.strip()
        if registrar:
            new_record[RecordAttribute.REGISTRAR] = registrar

        return new_record

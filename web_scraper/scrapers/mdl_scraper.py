# -*- coding: utf-8 -*-
"""

This module implements the Scraper interface for
Malware Domain List for data mining purposes.

"""

import logging
import urllib.error
import urllib.request

from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List

from scraper import Scraper


class MdlScraper(Scraper):
    """ Implementation of Scraper for MDL. """

    def __init__(self):
        # Specify the URL of MDL and its pages we are scanning.
        self.short_name = "mdl"
        self.base_url = "https://www.malwaredomainlist.com/"
        self.pages_scraped = ["hostslist/hosts.txt"] + \
                             ["mdl.php?page={0}".format(i) for i in range(23)]  # Pages are 0-22.
        super().__init__(self.short_name, self.base_url, self.pages_scraped)

        # Clear the remaining class attributes.
        self.records = {}
        self.timestamp = None

    def scrape(self):
        # Before beginning, clear all records and update the timestamp.
        self.records = {}
        self.timestamp = datetime.now()

        # Scrape the blacklisted hosts file first.
        self.records[self.pages_scraped[0]] = self._scrape_hosts()

        # Scrape all of the pages on the actual list.
        for page_name in self.pages_scraped[1:]:
            # Get a response from making a request to the page and only proceed if the request worked.
            url = self.base_url + page_name
            response, is_ok = self.make_request(url)
            if not is_ok:
                continue

            # Parse the response's content and store the records.
            html = BeautifulSoup(response.content, 'html.parser')
            self.records[page_name] = self._generate_records(url, html)
        logging.info("Completed scraping {0}".format(self.short_name))

    def _scrape_hosts(self) -> List[Dict]:
        """ Knowing that the first stored page is the hosts file, scrape records from it.

        :return: (list) A list of dictionaries where each dictionary is a record.
        """
        # Make a request to the URL and read one line at a time (first 6 line are useless).
        try:
            return [{'url': line.strip().decode('utf-8').split()[1]}
                    for line in list(urllib.request.urlopen(self.base_url+self.pages_scraped[0]))[6:]]
        except urllib.error.HTTPError as e:
            logging.error("Error when loading {0}: {1}".format(self.base_url+self.pages_scraped[0], e))
            return []

    @staticmethod
    def _generate_records(url: str, html: BeautifulSoup) -> List[Dict]:
        """ Given an MDL php page from the database, generate a list of records.

        :param url: (str) The current URL being viewed.
        :param html: (BeautifulSoup) HTML response from MDL.
        :return: (list) A list of record dictionaries.
        """
        info_table = html.find_all("table")[1]
        records = []
        for tr in info_table.select("tr")[2:]:
            # Find all tabular data. Make sure the data is how we expect it.
            td = tr.find_all("td")
            if len(td) != 7:
                logging.error("Data not formatted as expected for component in {0}".format(url))
                continue

            # Load information from the tabular data and store all of the loaded information into a new record.
            new_record = {
                "date": str(datetime.strptime(td[0].text, "%Y/%m/%d_%H:%M"))
            }

            if td[1].text != "-":
                new_record["url"] = td[1].text
                new_record["ip"] = td[2].text
            else:
                new_record["url"] = td[2].text
                new_record["ip"] = td[2].text.split("/")[0]

            if td[3].text != "-":
                new_record["reverse_lookup"] = td[3].text
            if td[4].txt != "-":
                new_record["description"] = td[4].txt
            if td[5].txt:
                new_record["asn"] = int(td[5].text)

            cc = td[6].find('img').get('alt')
            if cc:
                new_record["cc"] = cc

            records.append(new_record)
        return records

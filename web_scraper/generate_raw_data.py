#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""

This module is responsible for calling all scrapers and
generating raw information that precedes normalization.
Data which is scraped and processed is stored locally in
pickled files and JSONs.

"""

import importlib
import json
import logging
import os
import pickle

# Import all of the scrapers then clean up the imports.
import scrapers
all_scrapers = []
for module in scrapers.__all__:
    file = importlib.import_module(".{}".format(module), scrapers.__name__)
    scraper_class = getattr(file, "".join(word.capitalize() for word in module.split("_")))
    all_scrapers.append(scraper_class)
del scrapers


def reset_logging() -> None:
    """ Resets logging to use a local log file rather than the scrapers'.

    :return: None.
    """
    # Clear the root logger and make a formatter.
    root_logger = logging.getLogger()
    root_logger.handlers = []
    logging_format_string = '%(asctime)s %(levelname)s %(message)s'
    log_formatter = logging.Formatter(logging_format_string)

    # Create a file handler for this script.
    file_handler = logging.FileHandler('{0}/logs/generate_raw_data.log'
                                       .format(os.path.dirname(os.path.realpath(__file__))))
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)


if __name__ == "__main__":
    # Dump information from each of the scrapers.
    for scraper_class in all_scrapers:
        # Clear the logger, instantiate the scraper, scrape, and reset logging.
        logging.getLogger().handlers = []
        scraper = scraper_class()
        scraper.scrape()
        reset_logging()
        logging.info("Finished scraping {0}".format(scraper_class.__name__))

        # Aggregate all of the records.
        records_list = []
        for records in scraper.records.values():
            records_list.extend(records)

        # Save all of the data in pickled objects and JSON.
        with open('raw_output/pickle_data/' + scraper_class.__name__ + ".pickle", 'wb') as filename_data:
            pickle.dump(records_list, filename_data, protocol=pickle.HIGHEST_PROTOCOL)
        logging.info("Finished pickling {0}".format(scraper_class.__name__))
        with open('raw_output/json_data/' + scraper_class.__name__ + ".json", 'w') as filename_json:
            json.dump(records_list, filename_json)
        logging.info("Finished JSON encoding {0}".format(scraper_class.__name__))
    logging.info("Done processing all data!")

import pandas as pd
import numpy as np
import json
from pandas.io.json import json_normalize
import os
import csv
import ast

class ResponseAnalyzer(object):
    def __init__(self, inp, merged = False):
        """
        Initializes an analyzer for VirusTotal response csv,
        as outputted by csv_read.py.
        Args:
            inp (str): either a directory containing raw csvs, or a
                a single cleaned, merged csv.
            merged (bool): if inp is a directory, set merged to False.
                Else, set to true.
        """
        # NOTE: since the unrolling is slow, we would suggest running once,
        # printing to a csv, and loading it using merged = True
        self.input = inp
        if not merged:
            self.df = pd.DataFrame()
            for f in os.listdir(self.input):
                if f.endswith(".csv"):
                    path = self.input + "/" + f
                    currdf = pd.read_csv(path)
                    response_df = self.unroll_response_col(currdf)
                    #df1 = pd.DataFrame(currdf.pop('response').apply(pd.io.json.loads).values.tolist(), index=currdf.index)
                    #currdf = json_normalize(currdf["response"].to_dict())
                    currdf = currdf.merge(response_df, on ="i")
                    self.df = pd.concat([self.df, currdf])
            self.df.drop(["response"], axis = 1, inplace = True)
        else:
            self.df = pd.read_csv(self.input)

    def unroll_response_col(self, df):
        """Unrolls response column (with a json value) into individual columns."""
        outdf = pd.DataFrame()
        for index, row in df.iterrows():
            r_dict = ast.literal_eval(row["response"])
            currdf = json_normalize([r_dict])
            currdf["i"] = row["i"]
            outdf = pd.concat([outdf, currdf])
        return outdf

    def get_description(self, out_dir):
        """
        Gets a basic description of the combined dataframe.
        Currently, reads the summary statistics of the numerical
        fields and the unique values of the non-numerical fields
        to outdir.
        """
        describedf = self.df.describe(include=[np.number])
        describedf.to_csv(out_dir + "/hash_numerical.csv")
        unique = {}
        other_columns = self.df.select_dtypes(exclude=[np.number]).columns
        for col in other_columns:
            try:
                unique_vals = sorted(list(self.df[col].unique())[:100])
            except:
                unique_vals = list(self.df[col].unique())[:100]
            unique[col] = unique_vals
        with open(out_dir + "/hash_other.csv", "w") as csv_file:
            writer = csv.writer(csv_file)
            for key, val in unique.items():
                writer.writerow([key, val])
        print("Counts")
        print(self.df.count())

    def get_df(self):
        return self.df.copy()

    def print_csv(self, outfile):
        """Prints the dataframe to outfile."""
        self.df.to_csv(outfile, index = False)

    def get_duplicates_analysis(self):
        """
        Gets number of duplicates and reads all unique hashes
        into new csv.
        """
        seen_md5 = set()
        seen_sha256 = set()
        duplicates = 0
        unique_hash_ids = []
        for index, row in self.df.iterrows():
            if row['md5_x'] in seen_md5 or row['md5_y'] in seen_md5 or \
                row['sha256_x'] in seen_sha256 or row['sha256_y'] in seen_sha256:
                duplicates += 1
            else:
                nans = row.isnull()
                if not nans['md5_x']: seen_md5.add(row['md5_x'])
                if not nans['md5_y']: seen_md5.add(row['md5_y'])
                if not nans['sha256_x']: seen_sha256.add(row['sha256_x'])
                if not nans['sha256_y']: seen_sha256.add(row['sha256_y'])
                if not nans['md5_x'] or not nans['md5_y'] or \
                    not nans['sha256_x'] or not nans['sha256_y']:
                    unique_hash_ids.append(index)

        with open("./processed/hash_processed_no_duplicates.csv", "w") as csv_file:
            writer = csv.writer(csv_file)
            for key in unique_hash_ids:
                writer.writerow(self.df.iloc[key])
        print("Duplicates")
        print(duplicates)

    def get_real_malware_analysis(self):
        not_real_malware = 0
        for index, row in self.df.iterrows():
            if row['positives'] == 0:
                not_real_malware += 1
        print("Falsely Detected Malware")
        print(not_real_malware)

        
if __name__ == "__main__":
    # ra = ResponseAnalyzer("./processed/responses")
    # ra = ResponseAnalyzer("./processed/hash_processed.csv", merged=True)
    # ra.get_duplicates_analysis()
    # ra.get_description("./processed/descriptions")
    # ra.print_csv("./processed/hash_processed.csv")

    ra = ResponseAnalyzer("./processed/hash_processed_no_duplicates.csv", merged=True)
    ra.get_real_malware_analysis()
